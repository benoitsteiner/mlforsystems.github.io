---
title: Schedule
---
<div class="schedule_section">
  <div class="inner clearfix">
    <section class="main-content">
      <h2>Schedule</h2>
      <table class="schedule-table">
        <thead>
          <tr>
            <th style="text-align: center; white-space: nowrap;">Time</th>
            <th style="text-align: left">Section</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:00 AM</td>
            <td style="text-align: left">Opening Remarks</td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:10 AM</td>
            <td style="text-align: left"><a href="#schkufza_talk"><b>Invited Speaker 1: Eric Schkufza</b><br/><i>A Crash Course in Stochastic Program Optimization</i></a></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:35 AM</td>
            <td style="text-align: left"><a href="#song_han_talk"><b>Invited Speaker 2: Song Han</b><br/><i>Efficient deep learning computing: a learning-based approach</i></a></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">10:00 AM</td>
            <td style="text-align: left">Poster Session 1<br/></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">11:00 AM</td>
            <td style="text-align: left"><b>Caroline Lemieux</b><br/><i>Neural Inference of API Functions from Input–Output Examples</i> <a href="https://drive.google.com/file/d/1Gob8ePBJ9bkupFFiyEDJ7CED7Bupsvi9/view?usp=sharing">(slides)</a></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">11:15 AM</td>
            <td style="text-align: left">Placeto: Efficient Progressive Device Placement Optimization</td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">11:30 AM</td>
            <td style="text-align: left"><b>Fabian Ruffy</b><br/><i>Iroko: A Framework to Prototype Reinforcement Learning for Data Center Traffic Control</i> <a href="https://drive.google.com/file/d/14XnfsyhTISJWsquyVy-UoOgI08VwG0wa/view?usp=sharing">(slides)</a></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">11:45 AM</td>
            <td style="text-align: left"><a href="#partha_talk"><b>Invited Speaker 3: Partha Ranganathan</b><br/><i>ML for ML: Machine Learning to drive Moore's Law</i></a></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">12:10 PM</td>
            <td style="text-align: left">Lunch Break</td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">1:45 PM</td>
            <td style="text-align: left"><a href="#neeraja_talk"><b>Invited Speaker 4: Neeraja J. Yadwadkar</b><br/><i>Machine Learning for resource management in Distributed Systems</i></a></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">2:10 PM</td>
            <td style="text-align: left">Learning to Optimize Tensor Programs</td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">2:25 PM</td>
            <td style="text-align: left">Learning to Design Circuits</td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">2:40 PM</td>
            <td style="text-align: left">ReLeQ: A Reinforcement Learning Approach for Deep Quantization of Neural Networks</td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">2:55 PM</td>
            <td style="text-align: left">Poster Session 2<br/></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">4:00 PM</td>
            <td style="text-align: left"><a href="#sanjay_talk"><b>Invited Speaker 5: Sanjay Krishnan</b><br/><i>Learning to Optimize SQL Joins With Deep Reinforcement Learning</i></a> <a href="https://youtu.be/BZwDTKogS7U?t=2392">(recording)</a></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">4:25 PM</td>
            <td style="text-align: left"><b>Keynote Speaker: Jeff Dean</b><br/><i>Machine Learning for Systems</i> <a href="https://youtu.be/BZwDTKogS7U?t=3793">(recording)</a></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">4:50 PM</td>
            <td style="text-align: left"><b>Panel Discussion (Jeff Dean,  Partha Ranganathan, Song Han, and Ion Stoica)</b></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">5:50 PM</td>
            <td style="text-align: left">Closing Remarks (10mn)</td>
          </tr>
        </tbody>
      </table>
    </section>
  </div>
</div>
<div class="speaker_section">
  <div class="inner clearfix">
    <section class="main-content">
      <h2>Talk Abstracts</h2>
      <h3 id="schkufza_talk">A Crash Course in Stochastic Program Optimization</h3>
      <h5 class="talk_speaker">Eric Schkufza</h5>
      <p>
        Traditional compiler use expert-written rules to prove the correctness of program transformations, and hope for the best in terms of performance. Stochastic program optimizers turn that model on its head. They use machine learning techniques to search for aggressive performance-improving transformations, and state-of-the-art verification techniques to prove correctness after the fact. The results are novel, often inscrutable, and in many cases outperform expertly tuned code. In this talk I'll present an overview of the core technique, describe current work, and discuss directions for future research.
      </p>
      <h3 id="song_han_talk">Efficient deep learning computing: a learning-based approach</h3>
      <h5 class="talk_speaker">Song Han</h5>
      <p>
        In the post-Moore's Law era, the amount of computation per unit cost and power is no longer increasing at its historic rate. In the post-ImageNet era, researchers are solving more complicated AI problems using larger data sets which drives the demand for more computation.  This mismatch between supply and demand for computation highlights the need for co-designing efficient machine learning algorithms and domain-specific hardware architectures. Such algorithm-hardware co-design opens up a much larger design space, which requires domain experts on both sides (ML+systems), and human heuristics might be sub-optimal to explore the vast design space. We introduce three of our recent work of using machine learning to optimize the machine learning system: learning the optimal pruning strategy (<a href="https://arxiv.org/pdf/1802.03494.pdf">AMC</a>) and quantization strategy (<a href="https://arxiv.org/pdf/1811.08886.pdf">HAQ</a>) on the target hardware, rather than relying on rule-based strategies;  learning the optimal neural network architecture that is specialized for a target hardware architecture, optimizing both accuracy and latency (<a href="https://openreview.net/pdf?id=HylVB3AqYm">ProxylessNAS</a>), rather than using a generic neural network architecture across all hardware architectures; learning to optimize analog circuit parameters, rather than relying on experienced analog engineers to tune those transistors. On the other side of the loop (design hardware-friendly machine learning algorithms), I'll introduce the temporal shift module (<a href="https://hanlab.mit.edu/projects/tsm/">TSM</a>) that offers 8x lower latency, 12x higher throughput than 3D convolution-based methods, while ranking the first on both Something-Something V1 and V2 leaderboards. I'll conclude the talk by giving an outlook of the design automation for efficient machine learning system.
      </p>
      <h3 id="partha_talk">ML for ML: Machine Learning to drive Moore's Law</h3>
      <h5 class="talk_speaker">Partha Ranganathan</h5>
      <p>
        The computer architecture is facing an important and exciting challenge. The slowing of Moore's law (at the same time demand continues to grow) has led to new approaches to thinking about future system design including accelerators and software-defined hardware. In this talk we will discuss how machine learning has the potential to amplify these opportunities. We will discuss some specific case studies and end with some key insights specific to applying machine learning to improve computer architecture.
      </p>
      <h3 id="neeraja_talk">Machine Learning for resource management in Distributed Systems</h3>
      <h5 class="talk_speaker">Neeraja Yadwadkar</h5>
      <p>
        Traditional resource management techniques that rely on simple heuristics often fail to achieve predictable performance in contemporary complex systems that span physical servers, virtual servers, private and/or public clouds. My research aims to bring the benefits of Machine Learning (ML) models to optimize and manage such complex systems by deriving actionable insights from the performance and utilization data these systems generate. To realize this vision of model-based resource management, we need to deal with the following key challenges data-driven ML models raise: uncertainty in predictions, cost of training, generalizability from benchmark datasets to real-world systems datasets, and interpretability of the models.
      </p>
      <p>
        In this talk, I will present our the ML formulations to demonstrate how to handle these challenges for two main problem domains in distributed systems: (I) Scheduling in parallel data-intensive computational frameworks for improved tail latencies, and (II) Performance-aware resource allocation in the public cloud environments for meeting user-specified performance and cost goals. Along the way, I will also share a list of guidelines for leveraging ML for solving problems in systems, based on my experience.
      </p>
      <h3 id="sanjay_talk">Learning to Optimize SQL Joins With Deep Reinforcement Learning <a href="https://youtu.be/BZwDTKogS7U?t=2392">(recording)</a></h3>
      <h5 class="talk_speaker">Sanjay Krishnan</h5>
      <p>
        To integrate information from more than two tables, a SQL query optimizer must identify the most efficient nesting of two-way table join operations to answer the query. Recent advances in AI may provide an unexpected new perspective on this classical problem that has been studied for over 40 years. Join optimization can be posed as a Markov Decision Process where the state is a graph that represents the join conditions in a query and actions are edge contractions on this graph; thereby, allowing us to apply ideas from deep reinforcement learning and imitation learning to facilitate an improved query optimizer that learns from experience, handles uncertainty, and incorporates execution feedback. I describe how our group built a full-featured  query  optimizer  based on this MDP architecture, and we present results across a variety of database designs and query workloads in Postgres SQL and Apache Spark. I conclude by highlighting some of the under-appreciated RL research challenges in exploration, parametrization, and policy evaluation unearthed by this application.
      </p>
    </section>
  </div>
</div>
<div class="contact-us-section">
    <div class="inner clearfix">
        <section class="main-content">
            <h2>Contact Us</h2>
            <p>
                Contact us at <a href="mailto:mlforsystems@googlegroups.com">mlforsystems@googlegroups.com</a>.
            </p>
        </section>
    </div>
</div>
