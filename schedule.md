---
title: Schedule
workshop_name: isca2019
site_description: Workshop on ML for Systems at ISCA 2019, June 23rd, 9:00AM-5:00PM, Room 101A
site_title: ML For Systems
---

<div class="schedule_section">
  <div class="inner clearfix">
    <section class="main-content">
      <h2>Schedule</h2>
      <table class="schedule-table">
        <thead>
          <tr>
            <th style="text-align: center; white-space: nowrap;">Time</th>
            <th style="text-align: left">Section</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:00 AM</td>
            <td style="text-align: left"><b>Opening Remarks</b></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:10-9:40 AM</td>
            <td style="text-align: left"><b>Farinaz Koushanfar (UCSD)</b><br/></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:40-9:50 AM</td>
            <td style="text-align: left"><b>Talk 1</b><br/></td>
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:50-10:00 AM</td>
            <td style="text-align: left"><b>Talk 2</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">10:00-10:10 AM</td>
            <td style="text-align: left"><b>Talk 3</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">10:10-10:20 AM</td>
            <td style="text-align: left"><b>Talk 4</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">10:20-10:30 AM</td>
            <td style="text-align: left"><b>Talk 5</b><br/></td>
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:10-9:40 AM</td>
            <td style="text-align: left"><b>H.T. Kung (Harvard University)</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">11-11:30 AM</td>
            <td style="text-align: left"><b>Break</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">11:30-11:50 AM</td>
            <td style="text-align: left"><b>Zhan Shi (University of Texas at Austin)</b><br/></td>
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">11:50-12:10 PM</td>
            <td style="text-align: left"><b>Zhihao Jia (Stanford University)</b><br/></td>
          </tr>	
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">12:10-2:00 PM</td>
            <td style="text-align: left">Lunch<br/></td>
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">2:00-2:30 PM</td>
		  <td style="text-align: left"><b>Nathan Bechmann (CMU)</b><br/></td>
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">2:30-3:00 PM</td>
		  <td style="text-align: left"><b>Summer Deng (Facebook)</b><br/></td>
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">3:00-3:10 PM</td>
		  <td style="text-align: left"><b>Talk 6</b><br/></td>
          </tr>
	<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">3:10-3:20 PM</td>
		  <td style="text-align: left"><b>Talk 7</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">3:20-3:45 PM</td>
		  <td style="text-align: left"><b>Break</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">3:45-4:15 PM</td>
		  <td style="text-align: left"><b>David Patterson</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">4:15-4:50 PM</td>
		  <td style="text-align: left"><b>Panel Discussion</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">5:15 PM</td>
		  <td style="text-align: left">Turing Award Lecture (Symphony Hall)<br/></td>
          </tr>
        </tbody>
      </table>
    </section>
  </div>
</div>
<div class="speaker_section">
  <div class="inner clearfix">
    <section class="main-content">
      <h2 id="speakers">Speakers</h2>
	    <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/dave_patterson.jpg)"></div>
				<div>
					<h3>David Patterson (Google Brain)</h3>
<!-- 					<h5 class="keynote-speaker">Don’t Use a Single Large Systolic Array, Use Many Small Ones Instead</h5> -->
<!-- 					<p>
					    It is known that systolic arrays can efficiently implement matrix multiplications in deep learning computations. For high array utilization, we describe how we can (1) pack sparse filter matrices for small systolic arrays [ASPLOS 2019], and (2) employ small arrays to accommodate heterogeneous workloads [ASAP 2019], such as the Transformer for NLP.  Scheduling-wise, we will tile matrices according to the dimensions of the given systolic arrays and then pipe the
resulting tiles into the arrays for processing. We discuss an ongoing effort on the development of a parallel computing system composed of multiple memory and logic blocks as well as their switching structure in support of the “tile and pipe” computation model.
					</p> -->
				</div>
	</div>	    
      <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/htkungphoto.png)"></div>
				<div>
					<h3>H. T. Kung (Harvard University)</h3>
					<h5 class="keynote-speaker">Don’t Use a Single Large Systolic Array, Use Many Small Ones Instead</h5>
					<p>
					    It is known that systolic arrays can efficiently implement matrix multiplications in deep learning computations. For high array utilization, we describe how we can (1) pack sparse filter matrices for small systolic arrays [ASPLOS 2019], and (2) employ small arrays to accommodate heterogeneous workloads [ASAP 2019], such as the Transformer for NLP.  Scheduling-wise, we will tile matrices according to the dimensions of the given systolic arrays and then pipe the
resulting tiles into the arrays for processing. We discuss an ongoing effort on the development of a parallel computing system composed of multiple memory and logic blocks as well as their switching structure in support of the “tile and pipe” computation model.
					</p>
				</div>
	</div>
 <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/farinaz_koushanfar.jpeg)"></div>
				<div>
					<h3>Farinaz Koushanfar (UCSD)</h3>
<!-- 					<h5 class="keynote-speaker">Don’t Use a Single Large Systolic Array, Use Many Small Ones Instead</h5> -->
<!-- 					<p>
					    It is known that systolic arrays can efficiently implement matrix multiplications in deep learning computations. For high array utilization, we describe how we can (1) pack sparse filter matrices for small systolic arrays [ASPLOS 2019], and (2) employ small arrays to accommodate heterogeneous workloads [ASAP 2019], such as the Transformer for NLP.  Scheduling-wise, we will tile matrices according to the dimensions of the given systolic arrays and then pipe the
resulting tiles into the arrays for processing. We discuss an ongoing effort on the development of a parallel computing system composed of multiple memory and logic blocks as well as their switching structure in support of the “tile and pipe” computation model.
					</p> -->
				</div>
	    </div>
	    
 <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/nathan_beckmann.jpg)"></div>
				<div>
					<h3>Nathan Beckmann (CMU)</h3>
<!-- 					<h5 class="keynote-speaker">Don’t Use a Single Large Systolic Array, Use Many Small Ones Instead</h5> -->
<!-- 					<p>
					    It is known that systolic arrays can efficiently implement matrix multiplications in deep learning computations. For high array utilization, we describe how we can (1) pack sparse filter matrices for small systolic arrays [ASPLOS 2019], and (2) employ small arrays to accommodate heterogeneous workloads [ASAP 2019], such as the Transformer for NLP.  Scheduling-wise, we will tile matrices according to the dimensions of the given systolic arrays and then pipe the
resulting tiles into the arrays for processing. We discuss an ongoing effort on the development of a parallel computing system composed of multiple memory and logic blocks as well as their switching structure in support of the “tile and pipe” computation model.
					</p> -->
				</div>
	    </div>
<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/summer_deng.jpg)"></div>
				<div>
					<h3>Zhaoxia (Summer) Deng (Facebook)</h3>
<!-- 					<h5 class="keynote-speaker">Don’t Use a Single Large Systolic Array, Use Many Small Ones Instead</h5> -->
<!-- 					<p>
					    It is known that systolic arrays can efficiently implement matrix multiplications in deep learning computations. For high array utilization, we describe how we can (1) pack sparse filter matrices for small systolic arrays [ASPLOS 2019], and (2) employ small arrays to accommodate heterogeneous workloads [ASAP 2019], such as the Transformer for NLP.  Scheduling-wise, we will tile matrices according to the dimensions of the given systolic arrays and then pipe the
resulting tiles into the arrays for processing. We discuss an ongoing effort on the development of a parallel computing system composed of multiple memory and logic blocks as well as their switching structure in support of the “tile and pipe” computation model.
					</p> -->
				</div>
	    </div>
	<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/zhan_shi.jpeg)"></div>
				<div>
					<h3>Zhan Shi (University of Texas at Austin)</h3>
					<h5 class="keynote-speaker">Learning Execution through Neural Code Fusion</h5>
					<p>
					    As the performance of computer systems stagnates due to the end of Moore’s Law, there is a need for new models that can understand and optimize the execution of general purpose code. While there is a growing body of work on using Graph Neural Networks (GNNs) to learn representations of source code, these representations do not understand how code dynamically executes. In this work, we propose a new approach to use GNNs to learn fused representations of general source code and its execution. Our approach defines a multi-task GNN over low-level representations of source code and program state (i.e., assembly code and dynamic memory states), converting complex source code constructs and complex data structures into a simpler, more uniform format. We show that this leads to improved performance over similar methods that do not use execution and it opens the door to applying GNN models to new tasks that would not be feasible from static code alone.  As an illustration of this, we apply the new model to challenging dynamic tasks (branch prediction and prefetching) from the SPEC CPU benchmark suite, outperforming the state-of-the-art by 26% and 45% respectively. Moreover, we use the learned fused graph embeddings to demonstrate transfer learning with high performance
on an indirectly related task (algorithm classification).
					</p>
				</div>
	</div>
	<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/zhihao.png)"></div>
				<div>
					<h3>Zhihao Jia (Stanford University)</h3>
					<h5 class="keynote-speaker">Search-Based Approaches to Accelerate Deep Learning</h5>
					<p>
					    Current deep learning (DL) frameworks accelerate DL computation by applying a sequence of heuristic optimizations designed for common DNN models and hardware architectures. These frameworks generally miss subtle optimization opportunities that are specific to particular models and hardware. To address this limitation, I will present our recent work on designing search-based approaches to accelerate deep learning. To automatically optimize DNN computation on a specific hardware platform, we first design a comprehensive search space of possible deployment strategies, and use efficient search algorithms to discover optimized strategies in the search space. I will present two search-based DL systems: (1) FlexFlow automatically discovers fast strategies to parallelize DNN training, and outperforms existing data/model parallelism by up to 3.3x; and (2) XFlow is a DNN computation graph optimizer with automatically generated graph substitutions, which outperforms existing rule-based graph optimizers by up to 2.9x. I will conclude the talk by discussing the challenges and research opportunities in building end-to-end automated deep learning frameworks.
					</p>
				</div>
	</div>
<div class="contact-us-section">
    <div class="inner clearfix">
        <section class="main-content">
            <h2>Contact Us</h2>
            <p>
                Contact us at <a href="mailto:mlforsystems@googlegroups.com">mlforsystems@googlegroups.com</a>.
            </p>
        </section>
    </div>
</div>
