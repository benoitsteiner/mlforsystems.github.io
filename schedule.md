---
title: Schedule
workshop_name: isca2019
site_description: Workshop on ML for Systems at ISCA 2019, June 23rd, 9:00AM-5:00PM, Room 101A
site_title: ML For Systems
---

<div class="schedule_section">
  <div class="inner clearfix">
    <section class="main-content">
      <h2>Schedule</h2>
      <table class="schedule-table">
        <thead>
          <tr>
            <th style="text-align: center; white-space: nowrap;">Time</th>
            <th style="text-align: left">Section</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:00-9:10 AM</td>
            <td style="text-align: left"><b>Opening Remarks</b></td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:10-9:40 AM</td>
            <td style="text-align: left"><b>Automated Building of Safe and Robust Intelligent Systems</b><br><i>Keynote Speaker</i>: Farinaz Koushanfar - UCSD</td>
          </tr>
          <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:40-9:50 AM</td>
		  <td style="text-align: left"><b><a href="/assets/papers/isca2019/MLforSystems2019_Charith_Mendis.pdf">Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks</a></b><b>(Best Paper)</b><br/><i>Speaker</i>: Charith Mendis - MIT<br/><a href="/assets/slides/isca2019/MLforSystems2019_Talk_Charith_Mendis.key">Slides</a></td>  
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">9:50-10:00 AM</td>
            <td style="text-align: left"><b>Reinforcement Learning and Adaptive Sampling for Optimized DNN Compilation</b><br/><i>Speaker</i>: Byung Hoon Ahn - UCSD<br/><a href="/assets/slides/isca2019/MLforSystems2019_Talk_Byung_Hoon_Ahn.pptx">Slides</a></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">10:00-10:10 AM</td>
            <td style="text-align: left"><b><a href="/assets/papers/isca2019/MLforSystems2019_Mohammad_Samragh.pdf">AutoRank: Automated Rank Selection for Effective Neural Network Customization</a></b><br/><i>Speaker</i>: Mohammad Samragh - UCSD<br/><a href="/assets/slides/isca2019/MLforSystems2019_Talk_Mohammad_Samragh.pptx">Slides</a></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">10:10-10:20 AM</td>
            <td style="text-align: left"><b><a href="/assets/papers/isca2019/MLforSystems2019_Xiaoxi_Zhang.pdf">Optimal Learning-Based Network Protocol Selection</a></b><br/><i>Speaker</i>: Xiaoxi Zhang - CMU<br/><a href="/assets/slides/isca2019/MLforSystems2019_Talk_Xiaoxi_Zhang.pptx">Slides</a></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">10:20-10:30 AM</td>
            <td style="text-align: left"><b>Coda: An End-to-End Neural Program Decompiler</b><br/><i>Speaker</i>: Jishen Zhao - UCSD<br/><a href="/assets/slides/isca2019/MLforSystems2019_Talk_Cheng_Fu.pptx">Slides</a></td>
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">10:30-11:00 AM</td>
            <td style="text-align: left"><b>Don’t Use a Single Large Systolic Array, Use Many Small Ones Instead</b><br/><i>Keynote Speaker</i>: H.T. Kung - Harvard University<br/><a href="/assets/slides/isca2019/MLforSystems2019_HT_Kung.pdf">Slides</a></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">11-11:30 AM</td>
            <td style="text-align: left"><b>Break</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">11:30-11:55 AM</td>
            <td style="text-align: left"><b>Learning Execution through Neural Code Fusion</b><br/><i>Speaker</i>: Zhan Shi - University of Texas at Austin</td>
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">11:55-12:20 PM</td>
            <td style="text-align: left"><b>Search-Based Approaches to Accelerate Deep Learning</b><br/><i>Speaker</i>: Zhihao Jia - Stanford University</td>
          </tr>	
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">12:20-2:00 PM</td>
            <td style="text-align: left">Lunch<br/></td>
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">2:00-2:30 PM</td>
		  <td style="text-align: left"><b>Teaching an Old Cache New Tricks: Learning Better Caching Policies Online</b><br/><i>Speaker</i>: Nathan Beckmann - CMU<br/><a href="/assets/slides/isca2019/MLforSystems2019_Nathan_Beckmann.pptx">Slides</a></td>
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">2:30-3:00 PM</td>
		  <td style="text-align: left"><b>Deep Learning Acceleration via Low Precision Computing</b><br/><i>Speaker</i>: Summer Deng - Facebook<br/><a href="/assets/slides/isca2019/MLforSystems2019_Summer_Deng.pptx">Slides</a></td>			
          </tr>
	  <tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">3:00-3:10 PM</td>
		  <td style="text-align: left"><b><a href="/assets/papers/isca2019/MLforSystems2019_Ahmed_T_Elthakeb.pdf">SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training</a></b><br/><i>Speaker</i>: Ahmed Youssef - UCSD<br/><a href="/assets/slides/isca2019/MLforSystems2019_Talk_Ahmed_T_Elthakeb.pptx">Slides</a></td>
          </tr>
	<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">3:10-3:20 PM</td>
		  <td style="text-align: left"><b><a href="/assets/papers/isca2019/MLforSystems2019_Ajay_Jain.pdf">Learning Automatic Schedulers with Projective
			  Reparameterization</a></b><br/><i>Speaker</i>: Ajay Jain - UC Berkeley<br/><a href="/assets/slides/isca2019/MLforSystems2019_Talk_Ajay_Jain.pptx">Slides</a></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">3:20-3:45 PM</td>
		  <td style="text-align: left"><b>Break</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">3:45-4:15 PM</td>
		  <td style="text-align: left"><b>Domain-Specific Architectures for Deep Neural Networks</b><br/><i>Keynote Speaker</i>: David Patterson - Google Brain<br/><a href="/assets/slides/isca2019/MLforSystems2019_Dave_Patterson">Slides</a></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">4:15-5:00 PM</td>
		  <td style="text-align: left"><b>Panel Discussion</b><br/></td>
          </tr>
		<tr>
            <td style="text-align: right; white-space: nowrap; font-size: 15px;">5:15 PM</td>
		  <td style="text-align: left">Turing Award Lecture (Symphony Hall)<br/></td>
          </tr>
        </tbody>
      </table>
    </section>
  </div>
</div>
<div class="speaker_section">
  <div class="inner clearfix">
    <section class="main-content">
      <h2 id="speakers">Speakers</h2>
	    <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/dave_patterson.jpg)"></div>
				<div>
					<h3 class="keynote-speaker">Domain-Specific Architectures for Deep Neural Networks</h3>
					<h4>David Patterson - Google Brain</h4>
					<p>
					    With the ending of Moore's Law, many computer architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. The Tensor Processing Unit (TPU), deployed in Google datacenters since 2015, is a custom chip that accelerates deep neural networks (DNNs).  We compare the TPU to contemporary server-class CPUs and GPUs deployed in the same datacenters. Our benchmark workload, written using the high-level TensorFlow framework, uses production DNN applications that represent 95% of our datacenters’ DNN demand. The TPU is  an order of magnitude faster than contemporary CPUs and GPUs and its relative performance per Watt is even larger. We also describe the next two generations of TPUs, which are designed for training.
					</p>
				</div>
	</div>	    
      <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/htkungphoto.png)"></div>
				<div>					
					<h3 class="keynote-speaker">Don’t Use a Single Large Systolic Array, Use Many Small Ones Instead</h3>
					<h4>H. T. Kung - Harvard University</h4>
					<p>
					    It is known that systolic arrays can efficiently implement matrix multiplications in deep learning computations. For high array utilization, we describe how we can (1) pack sparse filter matrices for small systolic arrays [ASPLOS 2019], and (2) employ small arrays to accommodate heterogeneous workloads [ASAP 2019], such as the Transformer for NLP.  Scheduling-wise, we will tile matrices according to the dimensions of the given systolic arrays and then pipe the
resulting tiles into the arrays for processing. We discuss an ongoing effort on the development of a parallel computing system composed of multiple memory and logic blocks as well as their switching structure in support of the “tile and pipe” computation model.
					</p>
				</div>
	</div>
 <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/farinaz_koushanfar.jpeg)"></div>
				<div>
					<h3 class="keynote-speaker">Automated Building of Safe and Robust Intelligent Systems</h3>
					<h4>Farinaz Koushanfar - UCSD</h4>
<!-- 					<h5 class="keynote-speaker">Don’t Use a Single Large Systolic Array, Use Many Small Ones Instead</h5> -->
					<p>
					    The fourth industrial revolution shaped by the Machine Learning (ML) algorithms is underway. However, the widescale adoption of the emerging intelligent learning methodologies is hindered by security, privacy and safety considerations in sensitive scenarios such as smart transportation, health-care, warfare, and financial systems. In this talk, I advocate automated end-to-end co-design of algorithms, hardware, software, and data for building safe and assured machine learning systems. The presentation is centered on model explainability and internal characterization of the hierarchical learning models such as deep neural networks. I discuss important applications of the extracted characteristics in IP protection, Trojan detection, and thwarting of the adversarial attacks. These applications can be systematically and automatically customized for various platforms using our holistic end-to-end co-design methodology. I summarize by outlining the challenges and opportunities ahead.
					</p>
				</div>
	    </div>
	    
 <div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/nathan_beckmann.jpg)"></div>
				<div>
					<h3 class="keynote-speaker">Learn to Cache</h3>
					<h4>Nathan Beckmann - CMU</h4>
<!-- 					<h5 class="keynote-speaker">Don’t Use a Single Large Systolic Array, Use Many Small Ones Instead</h5> -->
					<p>
					    Caches are pervasive in computer systems and often determine overall system performance. Unfortunately, finding a caching policy that performs well is hard because applications vary so much in how they access data. Since traditional heuristics like recency and frequency leave too much performance on the table, caching policies are often hand-tuned in practice. But hand-tuning is unattractive because it takes significant effort and also makes caches fragile to changes in application behavior.

This talk will cover recent work on caching policies that learn & improve themselves online, without any hand-tuning or heuristics. Unsupervised reinforcement learning is not very effective in caching. To make a success of machine learning, caching systems must combine learning with the structure of caching problems. We will discuss two approaches: our recent work based on Bayesian inference, and policies that learn to imitate optimal cache replacement. We will show that these policies outperform the state-of-the-art by a large margin.
					</p>
				</div>
	    </div>
<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/summer_deng.jpg)"></div>
				<div>
					<h3 class="keynote-speaker">Deep Learning Acceleration via Low Precision Computing</h3>
					<h4>Zhaoxia (Summer) Deng - Facebook</h4>
<!-- 					<h5 class="keynote-speaker">Don’t Use a Single Large Systolic Array, Use Many Small Ones Instead</h5> -->
					<p>
					    Machine learning models are becoming more and more complicated and resource demanding, along with the fast growth of the deep learning area. These models are promising to achieve higher accuracies on various learning tasks, but the inference performance at realtime still needs to meet application-level latency or throughput requirements, in spite of practical constraints of available compute resources. Low-precision computing can effectively boost the inference efficiency while still maintaining similar learning accuracies. It can mitigate the memory bandwidth requirements and exploit the high performance of low-precision arithmetics on existing CPU platforms as well as future customized accelerators. In this talk, I'll present the low-precision computing techniques we have explored to optimize the deep learning workloads at facebook datacenters. Furthermore, I'll talk about opportunities for model co-design and guidance on the accelerator design exposed from numerical optimizations.
					</p>
				</div>
	    </div>
	<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/zhan_shi.jpeg)"></div>
				<div>
					<h3 class="keynote-speaker"><a href="https://arxiv.org/abs/1906.07181">Learning Execution through Neural Code Fusion</a></h3>
					<h4>Zhan Shi - University of Texas at Austin</h4>
					<p>
					    As the performance of computer systems stagnates due to the end of Moore’s Law, there is a need for new models that can understand and optimize the execution of general purpose code. While there is a growing body of work on using Graph Neural Networks (GNNs) to learn representations of source code, these representations do not understand how code dynamically executes. In this work, we propose a new approach to use GNNs to learn fused representations of general source code and its execution. Our approach defines a multi-task GNN over low-level representations of source code and program state (i.e., assembly code and dynamic memory states), converting complex source code constructs and complex data structures into a simpler, more uniform format. We show that this leads to improved performance over similar methods that do not use execution and it opens the door to applying GNN models to new tasks that would not be feasible from static code alone.  As an illustration of this, we apply the new model to challenging dynamic tasks (branch prediction and prefetching) from the SPEC CPU benchmark suite, outperforming the state-of-the-art by 26% and 45% respectively. Moreover, we use the learned fused graph embeddings to demonstrate transfer learning with high performance
on an indirectly related task (algorithm classification).
					</p>
				</div>
	</div>
	<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/zhihao.png)"></div>
				<div>
					<h3 class="keynote-speaker">Search-Based Approaches to Accelerate Deep Learning</h3>
					<h4>Zhihao Jia - Stanford University</h4>
					<p>
					    Current deep learning (DL) frameworks accelerate DL computation by applying a sequence of heuristic optimizations designed for common DNN models and hardware architectures. These frameworks generally miss subtle optimization opportunities that are specific to particular models and hardware. To address this limitation, I will present our recent work on designing search-based approaches to accelerate deep learning. To automatically optimize DNN computation on a specific hardware platform, we first design a comprehensive search space of possible deployment strategies, and use efficient search algorithms to discover optimized strategies in the search space. I will present two search-based DL systems: (1) FlexFlow automatically discovers fast strategies to parallelize DNN training, and outperforms existing data/model parallelism by up to 3.3x; and (2) XFlow is a DNN computation graph optimizer with automatically generated graph substitutions, which outperforms existing rule-based graph optimizers by up to 2.9x. I will conclude the talk by discussing the challenges and research opportunities in building end-to-end automated deep learning frameworks.
					</p>
				</div>
	</div>
<div class="contact-us-section">
    <div class="inner clearfix">
        <section class="main-content">
            <h2>Contact Us</h2>
            <p>
                Contact us at <a href="mailto:mlforsystems@googlegroups.com">mlforsystems@googlegroups.com</a>.
            </p>
        </section>
    </div>
</div>
