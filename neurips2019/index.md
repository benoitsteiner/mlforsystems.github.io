---
title: Announcement
workshop_name: neurips2019
site_description: Workshop on ML for Systems at NeurIPS 2019, December 14th, 9AM-6PM Vancouver Convention Centre, West Level 2 202-204
site_title: ML For Systems
---

<div class="inner clearfix">
	<section class="main-content overview_section">
		<p>
			Compute requirements are growing at an exponential rate<sup>1</sup>, and optimizing these computer systems often involves complex high-dimensional combinatorial problems. Yet, current methods rely heavily on heuristics. Very recent work has outlined a broad scope where machine learning vastly outperforms these traditional heuristics, including scheduling<sup>2,12</sup>, data structure design<sup>3,9</sup>, microarchitecture<sup>4</sup>, compilers<sup>5,8</sup>, circuit design<sup>7,10</sup>, and the control of warehouse scale computing systems<sup>6</sup>. In order to continue to scale these computer systems, new learning approaches are needed. The goal of this workshop is to develop novel machine learning methods to optimize and accelerate software and hardware systems.
		</p>
  	<p>
    		The main objective of this workshop is to expand upon this recent work and build a community focused on using machine learning in computer architecture and systems problems. We seek to improve the state of the art in the areas where learning has already proven to perform better than traditional heuristics, as well as expand to new areas throughout the system stack such as hardware/circuit design and operating/runtime systems.
  	</p>
  	<p>
  		We expect this year to improve the state of the art in areas where learning has already proven to outperform traditional heuristics. We also expect to expand into new areas throughout the systems stack, such as computer architecture and operating/runtime systems, and incorporate new ML topics like relational learning. Given that the community is larger than last year, for NeurIPS 2019, we intend to foster more discussion through breakout sessions. The interdisciplinary nature of this area makes NeurIPS an ideal venue for this workshop.
  	</p>
		<p>
			By forming a community of academic and industrial researchers who are excited about this area, we seek to build towards intelligent, self optimizing systems and answer questions such as: How do we generate and share high quality datasets that span the layers of the system stack? Which learned representations best represent code performance and runtime? Which simulators and simulation methodologies provide a tractable proving ground techniques like reinforcement learning?
		</p>
		<p>
			To this end, the target audience for this workshop includes a wide variety of attendees from state-of-the-art researchers in machine learning to domain experts in computer systems design. We have invited a <a href="#speakers">broad set of expert speakers</a> to present the potential for impact of combining deep learning research with computer systems. We hope that by providing a formal venue for researchers from both fields to meet and interact, that the result will include both fundamental research in ML as well as real-world impact to computer systems design and implementation.
		</p>
		<p>
			The workshop will host 6 speakers and we invite researchers to submit relevant papers through our <a href="/neurips2019/call_for_papers.html">call for papers</a>. The speakers, and potentially other relevant stakeholders, are invited to participate in a panel discussion to end the workshop. See the <a href="/neurips2019/schedule.html">schedule</a>.
		</p>
		<ul class="footnotes">
      <li><sup>1</sup> <a href="https://openai.com/blog/ai-and-compute/">AI and Compute</a></li>
      <li><sup>2</sup> <a href="https://arxiv.org/pdf/1706.04972.pdf">Device Placement Optimization with Reinforcement Learning</a></li>
      <li><sup>3</sup> <a href="https://arxiv.org/abs/1712.01208">The Case for Learned Index Structures</a></li>
      <li><sup>4</sup> <a href="https://arxiv.org/pdf/1803.02329.pdf">Learning Memory Access Patterns</a></li>
      <li><sup>5</sup> <a href="https://ieeexplore.ieee.org/document/8091247/?reload=true">End to End Deep Learning of Optimization Heuristics</a></li>
      <li><sup>6</sup> <a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">DeepMind AI Reduces Google Data Centre Cooling Bill by 40%</a></li>
      <li><sup>7</sup> <a href="https://arxiv.org/pdf/1903.00614.pdf">GAP: Generalizable Approximate Graph Partitioning Framework</a></li>
      <li><sup>8</sup> <a href="https://arxiv.org/abs/1808.07412">Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks</a></li>
      <li><sup>9</sup> <a href="https://arxiv.org/abs/1808.03196">Learning to Optimize Join Queries With Deep Reinforcement Learning</a></li>
      <li><sup>10</sup> <a href="https://arxiv.org/abs/1812.02734">Learning to Design Circuits</a></li>
      <li><sup>11</sup> <a href="https://arxiv.org/abs/1712.03890">DeepConf: Automating Data Center Network Topologies Management with Machine Learning.</a></li>
      <li><sup>12</sup> <a href="https://arxiv.org/abs/1810.01963">Scheduling Algorithms for Data Processing Clusters.</a></li>
    </ul>
	</section>
</div>
<div class="speaker_section">
	<div class="inner clearfix">
		<section class="main-content">
			<h2 id="speakers">Speakers</h2>
			<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/jeff_dean.jpg)"></div>
				<div>
					<h3>Jeff Dean</h3>
					<h5 class="keynote-speaker">Keynote Speaker</h5>
					<p>
					    Senior Fellow, Google AI. Google Brain lead and co-founder. Co-designer and implementor of Tensorflow, MapReduce, BigTable, Spanner.
					</p>
				</div>
			</div>
			<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/eytan_bakshy.jpg)"></div>
				<div>
					<h3>Eytan Bakshy</h3>
					<p>
					    Senior scientist on the Facebook Core Data Science Team where he leads the Adaptive Experimentation group. He is particularly interested in developing scalable and robust methods for sequential experimentation and reinforcement learning for real-world applications. In his former life, he was interested in substantive questions around the role of peer effects in online and offline behaviors, including information diffusion and civic engagement.
					</p>
				</div>
			</div>
			<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/akanksha_jain.jpg)"></div>
				<div>
					<h3>Akanksha Jain</h3>
					<p>
					    Research Associate at the Computer Science department of the University of Texas at Austin. She researches Computer Architecture, with a focus on memory system performance. Her research has introduced novel ways to improve hardware caching and prefetching. She is the inventor of the Hawkeye cache replacement policy, which won the 2017 Cache Replacement Championship. Her current research focus is to make machine learning a viable tool for computer architects.
					</p>
				</div>
			</div>
			<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/ion_stoica.jpg)"></div>
				<div>
					<h3>Ion Stoica</h3>
					<p>
					    Ion Stoica is a Professor in the EECS Department at University of California at Berkeley leading the RISELab. He does research on cloud computing and networked computer systems. Past work includes Apache Spark, Apache Mesos, Tachyon, Chord DHT, and Dynamic Packet State (DPS). He is an ACM Fellow and has received numerous awards, including the SIGOPS Hall of Fame Award (2015), the SIGCOMM Test of Time Award (2011), and the ACM doctoral dissertation award (2001). In 2013, he co-founded Databricks a startup to commercialize technologies for Big Data processing, and in 2006 he co-founded Conviva, a startup to commercialize technologies for large scale video distribution.
					</p>
				</div>
			</div>
			<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/mohammad_alizadeh.jpg)"></div>
				<div>
					<h3>Mohammad Alizadeh</h3>
					<p>
					    Assistant Professor in the EECS Department at MIT, and a member of CSAIL. He works in the areas of computer networks and systems. His research aims to improve the performance, robustness, and ease of management of future networks and cloud computing systems. His current research centers on network protocols and algorithms for large-scale datacenters, programmable switching architectures, and learning-based networked systems.
					</p>
				</div>
			</div>
			<!-- 
			<div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/vivienne_sze.jpg)"></div>
				<div>
					<h3>Vivienne Sze</h3>
					<p>
						Vivienne Sze is an Associate Professor in the Electrical Engineering and Computer Science Department at MIT. Her research interests include energy-efficient algorithms and architectures for portable multimedia applications. From September 2010 to July 2013, she was a Member of Technical Staff in the Systems and Applications R&D Center at Texas Instruments (TI), Dallas, TX, where she designed low-power algorithms and architectures for video coding. She also represented TI in the JCT-VC committee of ITU-T and ISO/IEC standards body during the development of High Efficiency Video Coding (HEVC), which received a Primetime Engineering Emmy Award. Within the committee, she was the primary coordinator of the core experiment on coefficient scanning and coding. She co-edited a book entitled High Efficiency Video Coding (HEVC) - Algorithms and Architecture (Springer, 2014).
					</p>
					<p>
						She is a recipient of the 2018 Facebook Faculty Award, the 2018 & 2017 Qualcomm Faculty Award, the 2018 & 2016 Google Faculty Research Award, the 2016 AFOSR Young Investigator Research Program (YIP) Award, the 2016 3M Non-Tenured Faculty Award, the 2014 DARPA Young Faculty Award, the 2007 DAC/ISSCC Student Design Contest Award, and a co-recipient of the 2018 Symposium on VLSI Circuits Best Student Paper Award, the 2017 CICC Outstanding Invited Paper Award, the 2016 IEEE Micro Top Picks Award and the 2008 A-SSCC Outstanding Design Award.
					</p>
				</div>
			</div><div class="speaker-bio">
				<div class="img-holder" style="background-image: url(/assets/images/speakers/oriol_vinyals.jpg)"></div>
				<div>
					<h3>Oriol Vinyals</h3>
					<p>
					    Oriol Vinyals is a Research Scientist at Google DeepMind, working in Deep Learning. Prior to joining DeepMind, Oriol was part of the Google Brain team. He holds a Ph.D. in EECS from University of California, Berkeley and is a recipient of the 2016 MIT TR35 innovator award. His research has been featured multiple times at the New York Times, BBC, etc., and his articles have been cited over 17000 times. His academic involvement includes program chair for the International Conference on Learning Representations (ICLR) of 2017, and 2018. He has also been an area chair for many editions of the NeurIPS and ICML conferences. At DeepMind he continues working on his areas of interest, which include artificial intelligence, with particular emphasis on machine learning, deep learning and reinforcement learning.
					</p>
				</div>
			</div>
		-->
		</section>
	</div>
</div>
<div class="organizers-section">
	<div class="inner clearfix">
		<section class="main-content">
			<h2>Organizing Committee</h2>
			<ul>
				<li><b>Anna Goldie</b>, Google Brain, <a href="https://twitter.com/annadgoldie">@annadgoldie</a></li>
				<li><b>Azalia Mirhoseini</b>, Google Brain, <a href="https://twitter.com/Azaliamirh">@Azaliamirh</a></li>
				<li><b>Jonathan Raiman</b>, OpenAI, <a href="https://twitter.com/jonathanrraiman">@jonathanrraiman</a></li>
				<li><b>Kevin Swersky</b>, Google Brain, <a href="https://twitter.com/kswersk">@kswersk</a></li>
				<li><b>Milad Hashemi</b>, Google, <a href="https://hps.ece.utexas.edu/people/miladh/">website</a></li>
				<li><b>Xinlei Xu</b>, NYU, <a href="https://twitter.com/MimeeXu">@MimeeXu</a></li>
			</ul>
			<h2>Program Committee</h2>
			<ul>
				<li><b>Michael Carbin</b>, MIT</li>
				<li><b>Carl Case</b>, NVIDIA</li>
				<li><b>Erich Elsen</b>, Google</li>
				<li><b>Andrew Gibiansky</b>, Voicery</li>
				<li><b>Anna Goldie</b>, Google Brain</li>
				<li><b>Rama Govindaraju</b>, Google</li>
				<li><b>Milad Hashemi</b>, Google</li>
				<li><b>Sara Hooker</b>, Google</li>
				<li><b>Safeen Huda</b>, Google</li>
				<li><b>Arpith Jacob</b>, Google</li>
				<li><b>Joe Jiang</b>, Google</li>
				<li><b>Derek Lockhart</b>, Google</li>
				<li><b>Martin Maas</b>, Google</li>
				<li><b>Sadhika Malladi</b>, Princeton</li>
				<li><b>Azalia Mirhoseini</b>, Google Brain</li>
				<li><b>Ashish Naik</b>, Google</li>
				<li><b>Azade Nazi</b>, Google Brain</li>
				<li><b>Aurojit Panda</b>, New York University Courant</li>
				<li><b>Jonathan Raiman</b>, OpenAI</li>
				<li><b>Herman Schmit</b>, Google</li>
				<li><b>Siddhartha Sen</b>, Microsoft Research</li>
				<li><b>Shubho Sengupta</b>, Facebook AI Research</li>
				<li><b>Narges Shahidi</b>, Google</li>
				<li><b>Zhan Shi</b>, University of Texas at Austin</li>
				<li><b>Tatiana Shpeisman</b>, Google</li>
				<li><b>Ebrahim Songhori</b>, Google</li>
				<li><b>Suvinay Subramanian</b>, Google</li>
				<li><b>Kevin Swersky</b>, Google Brain</li>
				<li><b>Phillipe Tillet</b>, Harvard</li>
				<li><b>Minjie Wang</b>, New York University</li>
				<li><b>John Whaley </b>, UnifyID Inc</li>
				<li><b>Qiumin Xu</b>, Google</li>
				<li><b>Xinlei Xu</b>, New York University</li>
				<li><b>Amir Yazdanbakhsh</b>, Google Research</li>
				<li><b>Dan Zhang</b>, Google X</li>
				<li><b>Yanqi Zhou</b>, Google</li>
			</ul>
			<h2>Contact Us</h2>
			<p>
				Contact us at <a href="mailto:mlforsystems@googlegroups.com">mlforsystems@googlegroups.com</a>.
			</p>
		</section>
</div>
